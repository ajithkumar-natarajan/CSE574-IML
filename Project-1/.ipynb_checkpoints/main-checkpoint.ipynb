{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "id": "7C9W9TxAlYFE",
    "outputId": "a661e957-2775-4d53-9ed0-969d03055cb3"
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "#Function to normalize feature values. Moves the point to origin and divides by range of the corresponding column.\n",
    "def normalize(data):\n",
    "    normalized_data = data.copy()\n",
    "    for column in data.columns:\n",
    "        max_value = data[column].max()\n",
    "        min_value = data[column].min()\n",
    "        normalized_data[column] = (data[column] - min_value) / (max_value - min_value)\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "#Sigmoid function definition which is the activation function.\n",
    "def sigmoid(z):\n",
    " return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\"\"\"Does the process of logistic regression. Takes the generic equation, estimates the probability of the cancer being malignant for the instance passed. The loss is evaluated which is subsequently used for updating the weights and bias.\"\"\"\n",
    "def logistic_regression(x, y, w, b, learning_rate):\n",
    "    m = x.shape[1]\n",
    "    z = np.dot(w.T, x) + b\n",
    "    a = sigmoid(z)\n",
    "    loss = -np.sum(np.multiply(np.log(a), y) + np.multiply((1 - y), np.log(1 - a)))/m\n",
    "    dz = a-y #Error in prediction is calculated.\n",
    "    dw = (1 / m) * np.dot(x, dz.T)\n",
    "    db = (1 / m) * np.sum(dz)\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    return loss, w, b, a\n",
    "\n",
    "# Reading the dataset  \n",
    "dataset = pd.read_csv(\"wdbc.dataset\", names=[\"ID\", \"Label\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\", \"X14\", \"X15\", \"X16\", \"X17\", \"X18\", \"X19\", \"X20\", \"X21\", \"X22\", \"X23\", \"X24\", \"X25\", \"X26\", \"X27\", \"X28\", \"X29\", \"X30\"])\n",
    "\n",
    "#Dropping patient ID and labels\n",
    "x = dataset.iloc[:, 2:] \n",
    "\n",
    "#Assigning label to y and mapping benign as '0' and malignant cases as '1'\n",
    "y = dataset['Label']\n",
    "y = y.map({'B': 0, 'M': 1})\n",
    "\n",
    "#Normalizing the dataset\n",
    "x = normalize(x)\n",
    "\n",
    "#Splitting the dataset into training (80%), validation (10%) and test (10%) data.\n",
    "x_train, x_remaining, y_train, y_remaining = train_test_split(x, y, train_size=0.8, random_state=10)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_remaining, y_remaining, test_size=0.5, random_state=10)\n",
    "\n",
    "\n",
    "#Flipping the data\n",
    "x_train, y_train = x_train.T, y_train.values.reshape(1, y_train.shape[0])\n",
    "x_val, y_val = x_val.T, y_val.values.reshape(1, y_val.shape[0])\n",
    "\n",
    "#Initializing parameters\n",
    "learning_rate = 10\n",
    "w = np.random.randn(x_train.shape[0], 1)*0.1\n",
    "b = 0\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "# training_accuracy = []\n",
    "# val_accuracy = []\n",
    "# output = []\n",
    "# training_prediction = []\n",
    "# val_prediction = []\n",
    "# training_accuracy = []\n",
    "# val_accuracy = []\n",
    "\n",
    "#Training model over 10000 epochs and calculating training loss and validation loss to study the training process\n",
    "for epoch in range(10000):\n",
    "    loss, w, b, a  = logistic_regression(x_train, y_train, w, b, learning_rate)\n",
    "    training_loss.append(np.squeeze(loss))\n",
    "#     aList = a.tolist()\n",
    "#     print((a[0][10]))\n",
    "#     y_train_list = y_train.tolist()\n",
    "#     for i in range(len(aList)):\n",
    "#       if(aList[0][i] <= 0.5):\n",
    "#           training_prediction.append(0)\n",
    "#       else:\n",
    "#           training_prediction.append(1)\n",
    "# #       print(aList[0][i])\n",
    "          \n",
    "#     count = 0\n",
    "#     for i in range(len(y_train_list)):\n",
    "#       if(y_train_list[i] == training_prediction[i]):\n",
    "#         count +=1\n",
    "#     acc = 100*count/y_train.shape[0]\n",
    "#     training_accuracy.append(acc)\n",
    "    \n",
    "    z = np.dot(w.T, x_val) + b\n",
    "    a = sigmoid(z)\n",
    "    m = x_val.shape[1]\n",
    "    loss = -np.sum(np.multiply(np.log(a), y_val) + np.multiply((1 - y_val), np.log(1 - a)))/m\n",
    "    validation_loss.append(np.squeeze(loss))\n",
    "#     aList = a.tolist()\n",
    "#     y_val_list = y_val.tolist()\n",
    "#     for i in range(len(aList)):\n",
    "#       if(aList[0][i] <= 0.5):\n",
    "#           val_prediction.append(0)\n",
    "#       else:\n",
    "#           val_prediction.append(1)\n",
    "#     count = 0\n",
    "#     for i in range(len(y_val_list)):\n",
    "#       if(y_val_list[i] == val_prediction[i]):\n",
    "#         count +=1\n",
    "#     acc = 100*count/y_val.shape[0]\n",
    "#     val_accuracy.append(acc)      \n",
    "\n",
    "\n",
    "#Loss vs epochs for training and validation data is plotted to observe the convergence\n",
    "plot.ylabel('Loss')\n",
    "plot.xlabel('Epochs')\n",
    "plot.title('Loss vs Epochs')\n",
    "plot.plot(training_loss, label='Training dataset', color='red')\n",
    "plot.plot(validation_loss, label='Validation dataset', color='blue')\n",
    "plot.legend(loc='best')\n",
    "# print(validation_loss[0])\n",
    "# print(validation_loss[-1])\n",
    "# print(training_loss[0])\n",
    "# print(training_loss[-1])\n",
    "# plt.savefig(\"O1.svg\")\n",
    "# print((training_accuracy))\n",
    "# plot.ylabel('Accuracy')\n",
    "# plot.xlabel('Epochs')\n",
    "# plot.title('Accuracy vs Epochs')\n",
    "# plot.plot(training_accuracy, label='Training dataset', color='red')\n",
    "# plot.plot(val_accuracy, label='Validation dataset', color='blue')\n",
    "# plot.legend(loc='best')\n",
    "# plt.savefig(\"O1.svg\")\n",
    "\n",
    "\n",
    "#Testing the model with unseen data to evaluate it\n",
    "z = np.dot(x_test, w) + b\n",
    "a = sigmoid(z)\n",
    "\n",
    "#Convert the probabilistic output to class\n",
    "for i in range(len(a)):\n",
    "    if(a[i] <= 0.5):\n",
    "        output.append(0)\n",
    "    else:\n",
    "        output.append(1)\n",
    "\n",
    "#Computation of evaluation parameters\n",
    "y_test = y_test.tolist()\n",
    "predicted_labels = output\n",
    "\n",
    "confusion_matrix = confusion_matrix(actual, predicted)\n",
    "print(confusion_matrix)\n",
    "\n",
    "tp = confusion_matrix[0][0]\n",
    "fp = confusion_matrix[0][1]\n",
    "fn = confusion_matrix[1][0]\n",
    "tn = confusion_matrix[1][1]\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "print('Precision:', precision)\n",
    "\n",
    "recall = tp/(tp+fn)\n",
    "print('Recall:', recall)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
