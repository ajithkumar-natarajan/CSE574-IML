{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9hMP_zftRIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d9abebb-c405-4d63-a148-0649c55123dd"
      },
      "source": [
        "\"\"\"\n",
        "Created on 10/17/19 @ 16:54:10\n",
        "\n",
        "@author: ajithkumar-natarajan\n",
        "\"\"\"\n",
        "\n",
        "##############################TASK 1\n",
        "\n",
        "#Importing necessary libraries\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "\n",
        "\n",
        "#Function to load data\n",
        "def load_mnist(path, kind='train'):\n",
        "  import os\n",
        "  import gzip\n",
        "\n",
        "  \"\"\"Load MNIST data from `path`\"\"\"\n",
        "  labels_path = os.path.join(path,\n",
        "                                '%s-labels-idx1-ubyte.gz'\n",
        "                                % kind)\n",
        "  images_path = os.path.join(path,\n",
        "                                '%s-images-idx3-ubyte.gz'\n",
        "                                % kind)\n",
        "  with gzip.open(labels_path, 'rb') as lbpath:\n",
        "    labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                                offset=8)\n",
        "\n",
        "  with gzip.open(images_path, 'rb') as imgpath:\n",
        "    images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                                offset=16).reshape(len(labels), 784)\n",
        "\n",
        "  return images, labels\n",
        "  \n",
        "#Sigmoid function \n",
        "def sigmoid(z):\n",
        "  a = 1/(1 + np.exp(-z))\n",
        "  return a\n",
        "\n",
        "\n",
        "#Sigmoid derivative \n",
        "def sigmoid_derivative(a):\n",
        "  return a*(1 - a)\n",
        "\n",
        "\n",
        "#Softmax function\n",
        "def softmax(x):\n",
        "  a = np.exp(x - np.max(x))\n",
        "  return a / a.sum(axis = 1, keepdims = True)\n",
        "\n",
        "\n",
        "#Loss function \n",
        "def compute_loss(a, y):\n",
        "  return np.sum(-np.log(a[range(y.shape[0]), y]))/y.shape[0]\n",
        "\n",
        "\n",
        "#Reading input data\n",
        "x_train, y_train = load_mnist('data/fashion', kind='train')\n",
        "x_test, y_test = load_mnist('data/fashion', kind='t10k')\n",
        "\n",
        "\n",
        "#Normalizing input data\n",
        "normalized_x_train = x_train/255\n",
        "normalized_x_test = x_test/255\n",
        "\n",
        "\n",
        "#Initializing values\n",
        "x1 = normalized_x_train\n",
        "x2 = normalized_x_test\n",
        "y1 = y_train\n",
        "y1_test = y_test\n",
        "epochs = 1000\n",
        "learning_rate = 0.01\n",
        "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
        "y2 = y_train.reshape(len(y_train), 1)\n",
        "y2_test = y_test.reshape(len(y_test), 1)\n",
        "y2 = onehot_encoder.fit_transform(y2)\n",
        "y2_test = onehot_encoder.fit_transform(y2_test)\n",
        "m = y1.shape[0]\n",
        "no_of_hidden_nodes = 128\n",
        "w1 = np.random.randn(x1.T.shape[0], no_of_hidden_nodes) \n",
        "w2 = np.random.randn(no_of_hidden_nodes, 10) \n",
        "b1 = np.zeros(no_of_hidden_nodes)\n",
        "b2 = np.zeros(10)    \n",
        "training_loss = list()\n",
        "validation_loss = list()\n",
        "\n",
        "    \n",
        "for epoch in range(epochs):\n",
        "  #Foward propogation\n",
        "  z1 = np.matmul(x1, w1)+ b1\n",
        "  a1 = sigmoid(z1)\n",
        "  z2 = np.matmul(a1, w2)+ b2\n",
        "  a2 = softmax(z2)\n",
        "  final_loss = compute_loss(a2, y1)\n",
        "  print(\"Loss for epoch\", epoch+1,\":\", final_loss)\n",
        "  training_loss.append(final_loss)\n",
        "  \n",
        "  #Back propogation\n",
        "  del_z2 = a2 - y2\n",
        "  del_w2 = (1/m) * np.dot(a1.T, del_z2)\n",
        "  del_b2 = (1/m) * np.sum(del_z2,axis = 0, keepdims = True)\n",
        "  \n",
        "  del_a1 = np.dot(del_z2, w2.T)\n",
        "  del_z1 = del_a1 * sigmoid_derivative(a1)\n",
        "  del_w1 = (1/m) * np.dot(x1.T, del_z1)\n",
        "  del_b1 = (1/m) * np.sum(del_z1, axis = 0, keepdims = True)\n",
        "  \n",
        "  w1 = w1 - learning_rate * del_w1\n",
        "  b1 = b1 - learning_rate * del_b1\n",
        "  w2 = w2 - learning_rate * del_w2\n",
        "  b2 = b2 - learning_rate * del_b2\n",
        "\n",
        "  z1_test = np.matmul(x2, w1)+ b1\n",
        "  a1_test = sigmoid(z1_test)\n",
        "  z2_test = np.matmul(a1_test, w2)+ b2\n",
        "  a2_test = softmax(z2_test)\n",
        "  final_loss = compute_loss(a2_test, y1_test)\n",
        "  validation_loss.append(final_loss)\n",
        "\n",
        "#Test data prediction\n",
        "acc = 0\n",
        "y_pred = list()\n",
        "for i,j in itertools.zip_longest(normalized_x_test, y_test):\n",
        "  z1 = np.dot(i, w1) + b1\n",
        "  a1 = sigmoid(z1)            \n",
        "  z2 = np.dot(a1, w2) + b2\n",
        "  a2 = softmax(z2)\n",
        "  prediction = np.argmax(a2)\n",
        "  y_pred.append(prediction)\n",
        "  if prediction == j:\n",
        "    acc = acc+1\n",
        "accuracy = (acc / normalized_x_test.shape[0])*100\n",
        "print(\"Test Accuracy:\", accuracy, \"%\")\n",
        "\n",
        "#Generating confusion matrix\n",
        "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(confusion_matrix)\n",
        "\n",
        "#Plotting graphs\n",
        "fig = plt.gcf()\n",
        "plt.plot(training_loss, '--m', linestyle='--', markersize=1)\n",
        "plt.plot(validation_loss, '--b', linestyle='--', markersize=1)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "# fig.savefig('training_validation_loss_task1.png', dpi=100)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#############################TASK 2\n",
        "\n",
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# LOG_DIR = './log'\n",
        "# get_ipython().system_raw(\n",
        "#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "#     .format(LOG_DIR)\n",
        "# )\n",
        "\n",
        "# get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n",
        "#necessary imports\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "# import pickle\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "\n",
        "# def load_mnist(path, kind='train'):\n",
        "# \timport os\n",
        "# \timport gzip\n",
        "# \timport numpy as np\n",
        "\n",
        "# \t\"\"\"Load MNIST data from `path`\"\"\"\n",
        "# \tlabels_path = os.path.join(path,\n",
        "#                                '%s-labels-idx1-ubyte.gz'\n",
        "#                                % kind)\n",
        "# \timages_path = os.path.join(path,\n",
        "#                                '%s-images-idx3-ubyte.gz'\n",
        "#                                % kind)\n",
        "# \twith gzip.open(labels_path, 'rb') as lbpath:\n",
        "# \t\tlabels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "#                                offset=8)\n",
        "\n",
        "# \twith gzip.open(images_path, 'rb') as imgpath:\n",
        "# \t\timages = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "#                                offset=16).reshape(len(labels), 784)\n",
        "\n",
        "# \treturn images, labels\n",
        "\n",
        "# x_train, y_train = load_mnist('data/fashion', kind='train')\n",
        "# x_test, y_test = load_mnist('data/fashion', kind='t10k')\n",
        "\n",
        "#Loading dataset from keras\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "#setting batch size\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "# print(type(x_train))\n",
        "\n",
        "# x_train = x_train.reshape(x_train, (60000, 28, 28 ))\n",
        "# x_test = x_test.reshape(x_test, (60000, 28, 28))\n",
        "\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "\n",
        "# i = 13\n",
        "# plt.imshow(x_train[i,:,:], cmap = matplotlib.cm.binary)\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()\n",
        "# print(\"label-> \", y_train[i])\n",
        "\n",
        "#normalizing data\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "\n",
        "#One-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_orig = y_test\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "#Stacking up layers for neural network\n",
        "model = keras.Sequential([keras.layers.Flatten(input_shape=(28,28)), \n",
        "                          # keras.layers.Dense(400, activation='sigmoid'), \n",
        "                          keras.layers.Dense(128, activation='relu'), \n",
        "                          keras.layers.Dense(10, activation='softmax')])\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# tbCallBack = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "#                          write_graph=True,\n",
        "#                          write_grads=True,\n",
        "#                          batch_size=batch_size,\n",
        "#                          write_images=True)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=batch_size, validation_data=(x_test, y_test))\n",
        "# with open('trainHistoryDict', 'wb') as file_pi:\n",
        "        # pickle.dump(output.history, file_pi)\n",
        "\n",
        "output = model.predict_classes(x_test)\n",
        "# con_mat = tf.math.confusion_matrix(labels=y_test, predictions=output).numpy()\n",
        "# con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        " \n",
        "# con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "#                      index = classes, \n",
        "#                      columns = classes)\n",
        "\n",
        "#Printing confusion matrix\n",
        "confusion_matrix = confusion_matrix(y_test_orig, output)\n",
        "print(confusion_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#############################TASK 3\n",
        "#Importing necessaryy libraries\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plot\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# def load_mnist(path, kind='train'):\n",
        "# \timport os\n",
        "# \timport gzip\n",
        "\n",
        "# \t\"\"\"Load MNIST data from `path`\"\"\"\n",
        "# \tlabels_path = os.path.join(path,\n",
        "#                                '%s-labels-idx1-ubyte.gz'\n",
        "#                                % kind)\n",
        "# \timages_path = os.path.join(path,\n",
        "#                                '%s-images-idx3-ubyte.gz'\n",
        "#                                % kind)\n",
        "# \twith gzip.open(labels_path, 'rb') as lbpath:\n",
        "# \t\tlabels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "#                                offset=8)\n",
        "\n",
        "# \twith gzip.open(images_path, 'rb') as imgpath:\n",
        "# \t\timages = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "#                                offset=16).reshape(len(labels), 784)\n",
        "\n",
        "# \treturn images, labels\n",
        "\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "#Loading dataset\n",
        "x_train, y_train = load_mnist('data/fashion', kind='train')\n",
        "x_test, y_test = load_mnist('data/fashion', kind='t10k')\n",
        "\n",
        "#Getting data ready for CNN training\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_orig = y_test\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "#Building the CNN model\n",
        "model = keras.Sequential([keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape = (28, 28, 1)), \n",
        "                          keras.layers.pooling.MaxPooling2D(pool_size = (2, 2)), \n",
        "                          keras.layers.Conv2D(64, (3, 3), activation='relu'), \n",
        "                          keras.layers.pooling.MaxPool2D(pool_size = (2, 2)), \n",
        "                          keras.layers.Dropout(0.25), \n",
        "                          keras.layers.Flatten(), \n",
        "                          keras.layers.Dense(256, activation='relu'), \n",
        "                          keras.layers.Dropout(0.5), \n",
        "                          keras.layers.Dense(10, activation='softmax')])\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "#Predicting output\n",
        "output = model.predict_classes(x_test)\n",
        "\n",
        "#Printing confusion matrix\n",
        "confusion_matrix = confusion_matrix(y_test_orig, output)\n",
        "print(confusion_matrix)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss for epoch 1 : 10.239743661263306\n",
            "Loss for epoch 2 : 9.890990538284619\n",
            "Loss for epoch 3 : 9.562336090463672\n",
            "Loss for epoch 4 : 9.254613444095202\n",
            "Loss for epoch 5 : 8.968235214983347\n",
            "Loss for epoch 6 : 8.703174332459529\n",
            "Loss for epoch 7 : 8.458988652730532\n",
            "Loss for epoch 8 : 8.234877864260298\n",
            "Loss for epoch 9 : 8.029759201483634\n",
            "Loss for epoch 10 : 7.842350753217108\n",
            "Loss for epoch 11 : 7.671253254858079\n",
            "Loss for epoch 12 : 7.515022908640845\n",
            "Loss for epoch 13 : 7.372230500001808\n",
            "Loss for epoch 14 : 7.241505343023014\n",
            "Loss for epoch 15 : 7.121564847736486\n",
            "Loss for epoch 16 : 7.011231614805796\n",
            "Loss for epoch 17 : 6.909440531549378\n",
            "Loss for epoch 18 : 6.815238580655271\n",
            "Loss for epoch 19 : 6.727779902790939\n",
            "Loss for epoch 20 : 6.646318143478911\n",
            "Loss for epoch 21 : 6.570197499354953\n",
            "Loss for epoch 22 : 6.498843361467098\n",
            "Loss for epoch 23 : 6.431753104823414\n",
            "Loss for epoch 24 : 6.368487363135955\n",
            "Loss for epoch 25 : 6.308661993422263\n",
            "Loss for epoch 26 : 6.25194083580294\n",
            "Loss for epoch 27 : 6.198029297341161\n",
            "Loss for epoch 28 : 6.1466687348353215\n",
            "Loss for epoch 29 : 6.097631578844067\n",
            "Loss for epoch 30 : 6.050717125346258\n",
            "Loss for epoch 31 : 6.005747916740874\n",
            "Loss for epoch 32 : 5.962566635825651\n",
            "Loss for epoch 33 : 5.921033441989623\n",
            "Loss for epoch 34 : 5.88102368629198\n",
            "Loss for epoch 35 : 5.84242595015877\n",
            "Loss for epoch 36 : 5.805140360247893\n",
            "Loss for epoch 37 : 5.769077139081929\n",
            "Loss for epoch 38 : 5.734155357114092\n",
            "Loss for epoch 39 : 5.700301856996507\n",
            "Loss for epoch 40 : 5.6674503251005035\n",
            "Loss for epoch 41 : 5.635540488957032\n",
            "Loss for epoch 42 : 5.604517422375453\n",
            "Loss for epoch 43 : 5.574330942656873\n",
            "Loss for epoch 44 : 5.544935086609502\n",
            "Loss for epoch 45 : 5.516287654046794\n",
            "Loss for epoch 46 : 5.488349809143727\n",
            "Loss for epoch 47 : 5.46108573147721\n",
            "Loss for epoch 48 : 5.434462309813725\n",
            "Loss for epoch 49 : 5.408448872758393\n",
            "Loss for epoch 50 : 5.383016951268607\n",
            "Loss for epoch 51 : 5.358140068783712\n",
            "Loss for epoch 52 : 5.333793555349919\n",
            "Loss for epoch 53 : 5.309954382644528\n",
            "Loss for epoch 54 : 5.286601017242553\n",
            "Loss for epoch 55 : 5.263713289836845\n",
            "Loss for epoch 56 : 5.241272278432976\n",
            "Loss for epoch 57 : 5.2192602038034455\n",
            "Loss for epoch 58 : 5.197660335711146\n",
            "Loss for epoch 59 : 5.176456908606353\n",
            "Loss for epoch 60 : 5.1556350456701\n",
            "Loss for epoch 61 : 5.1351806902236286\n",
            "Loss for epoch 62 : 5.115080543651496\n",
            "Loss for epoch 63 : 5.095322009097454\n",
            "Loss for epoch 64 : 5.0758931402891205\n",
            "Loss for epoch 65 : 5.056782594931508\n",
            "Loss for epoch 66 : 5.03797959218222\n",
            "Loss for epoch 67 : 5.019473873783731\n",
            "Loss for epoch 68 : 5.001255668482295\n",
            "Loss for epoch 69 : 4.983315659409213\n",
            "Loss for epoch 70 : 4.9656449541401315\n",
            "Loss for epoch 71 : 4.948235057181989\n",
            "Loss for epoch 72 : 4.931077844666438\n",
            "Loss for epoch 73 : 4.91416554105347\n",
            "Loss for epoch 74 : 4.897490697670311\n",
            "Loss for epoch 75 : 4.881046172928886\n",
            "Loss for epoch 76 : 4.864825114080816\n",
            "Loss for epoch 77 : 4.848820940382408\n",
            "Loss for epoch 78 : 4.8330273275538\n",
            "Loss for epoch 79 : 4.817438193426599\n",
            "Loss for epoch 80 : 4.802047684683401\n",
            "Loss for epoch 81 : 4.7868501646004935\n",
            "Loss for epoch 82 : 4.771840201712313\n",
            "Loss for epoch 83 : 4.75701255932263\n",
            "Loss for epoch 84 : 4.742362185793435\n",
            "Loss for epoch 85 : 4.727884205547895\n",
            "Loss for epoch 86 : 4.713573910728768\n",
            "Loss for epoch 87 : 4.6994267534583205\n",
            "Loss for epoch 88 : 4.685438338650003\n",
            "Loss for epoch 89 : 4.671604417326175\n",
            "Loss for epoch 90 : 4.6579208803997725\n",
            "Loss for epoch 91 : 4.644383752881161\n",
            "Loss for epoch 92 : 4.6309891884746115\n",
            "Loss for epoch 93 : 4.617733464531617\n",
            "Loss for epoch 94 : 4.604612977330922\n",
            "Loss for epoch 95 : 4.591624237657617\n",
            "Loss for epoch 96 : 4.57876386665579\n",
            "Loss for epoch 97 : 4.566028591931371\n",
            "Loss for epoch 98 : 4.553415243883608\n",
            "Loss for epoch 99 : 4.540920752245425\n",
            "Loss for epoch 100 : 4.528542142814455\n",
            "Loss for epoch 101 : 4.516276534358013\n",
            "Loss for epoch 102 : 4.504121135676679\n",
            "Loss for epoch 103 : 4.492073242812326\n",
            "Loss for epoch 104 : 4.480130236387683\n",
            "Loss for epoch 105 : 4.468289579065513\n",
            "Loss for epoch 106 : 4.456548813116557\n",
            "Loss for epoch 107 : 4.444905558086287\n",
            "Loss for epoch 108 : 4.433357508551439\n",
            "Loss for epoch 109 : 4.421902431958105\n",
            "Loss for epoch 110 : 4.410538166533979\n",
            "Loss for epoch 111 : 4.39926261926808\n",
            "Loss for epoch 112 : 4.388073763952024\n",
            "Loss for epoch 113 : 4.376969639277558\n",
            "Loss for epoch 114 : 4.3659483469857285\n",
            "Loss for epoch 115 : 4.355008050063625\n",
            "Loss for epoch 116 : 4.344146970985255\n",
            "Loss for epoch 117 : 4.333363389993537\n",
            "Loss for epoch 118 : 4.3226556434209344\n",
            "Loss for epoch 119 : 4.312022122046633\n",
            "Loss for epoch 120 : 4.301461269488605\n",
            "Loss for epoch 121 : 4.290971580629161\n",
            "Loss for epoch 122 : 4.280551600072938\n",
            "Loss for epoch 123 : 4.2701999206365375\n",
            "Loss for epoch 124 : 4.259915181869177\n",
            "Loss for epoch 125 : 4.249696068603983\n",
            "Loss for epoch 126 : 4.239541309539603\n",
            "Loss for epoch 127 : 4.2294496758520195\n",
            "Loss for epoch 128 : 4.219419979836456\n",
            "Loss for epoch 129 : 4.209451073579383\n",
            "Loss for epoch 130 : 4.19954184766063\n",
            "Loss for epoch 131 : 4.189691229885644\n",
            "Loss for epoch 132 : 4.179898184047969\n",
            "Loss for epoch 133 : 4.170161708721934\n",
            "Loss for epoch 134 : 4.160480836085607\n",
            "Loss for epoch 135 : 4.150854630773984\n",
            "Loss for epoch 136 : 4.141282188762351\n",
            "Loss for epoch 137 : 4.131762636279757\n",
            "Loss for epoch 138 : 4.12229512875243\n",
            "Loss for epoch 139 : 4.11287884977698\n",
            "Loss for epoch 140 : 4.103513010123144\n",
            "Loss for epoch 141 : 4.0941968467658025\n",
            "Loss for epoch 142 : 4.084929621945965\n",
            "Loss for epoch 143 : 4.075710622260335\n",
            "Loss for epoch 144 : 4.06653915777907\n",
            "Loss for epoch 145 : 4.0574145611913055\n",
            "Loss for epoch 146 : 4.048336186977958\n",
            "Loss for epoch 147 : 4.039303410611306\n",
            "Loss for epoch 148 : 4.030315627780852\n",
            "Loss for epoch 149 : 4.021372253644869\n",
            "Loss for epoch 150 : 4.012472722107125\n",
            "Loss for epoch 151 : 4.003616485118155\n",
            "Loss for epoch 152 : 3.9948030120005265\n",
            "Loss for epoch 153 : 3.986031788797486\n",
            "Loss for epoch 154 : 3.9773023176443814\n",
            "Loss for epoch 155 : 3.9686141161622674\n",
            "Loss for epoch 156 : 3.959966716873079\n",
            "Loss for epoch 157 : 3.9513596666357786\n",
            "Loss for epoch 158 : 3.9427925261028727\n",
            "Loss for epoch 159 : 3.934264869196698\n",
            "Loss for epoch 160 : 3.9257762826049154\n",
            "Loss for epoch 161 : 3.9173263652945947\n",
            "Loss for epoch 162 : 3.908914728044351\n",
            "Loss for epoch 163 : 3.900540992993949\n",
            "Loss for epoch 164 : 3.8922047932108375\n",
            "Loss for epoch 165 : 3.8839057722730446\n",
            "Loss for epoch 166 : 3.875643583867943\n",
            "Loss for epoch 167 : 3.8674178914063075\n",
            "Loss for epoch 168 : 3.8592283676512045\n",
            "Loss for epoch 169 : 3.8510746943611687\n",
            "Loss for epoch 170 : 3.8429565619472026\n",
            "Loss for epoch 171 : 3.8348736691431076\n",
            "Loss for epoch 172 : 3.826825722688673\n",
            "Loss for epoch 173 : 3.8188124370252714\n",
            "Loss for epoch 174 : 3.8108335340034136\n",
            "Loss for epoch 175 : 3.8028887426018114\n",
            "Loss for epoch 176 : 3.7949777986575453\n",
            "Loss for epoch 177 : 3.7871004446068963\n",
            "Loss for epoch 178 : 3.779256429236464\n",
            "Loss for epoch 179 : 3.771445507444157\n",
            "Loss for epoch 180 : 3.7636674400096854\n",
            "Loss for epoch 181 : 3.7559219933741836\n",
            "Loss for epoch 182 : 3.7482089394285993\n",
            "Loss for epoch 183 : 3.740528055310506\n",
            "Loss for epoch 184 : 3.7328791232089977\n",
            "Loss for epoch 185 : 3.725261930177342\n",
            "Loss for epoch 186 : 3.717676267953073\n",
            "Loss for epoch 187 : 3.7101219327852233\n",
            "Loss for epoch 188 : 3.702598725268395\n",
            "Loss for epoch 189 : 3.6951064501833897\n",
            "Loss for epoch 190 : 3.6876449163441274\n",
            "Loss for epoch 191 : 3.6802139364505804\n",
            "Loss for epoch 192 : 3.6728133269474763\n",
            "Loss for epoch 193 : 3.665442907888534\n",
            "Loss for epoch 194 : 3.658102502805978\n",
            "Loss for epoch 195 : 3.6507919385851237\n",
            "Loss for epoch 196 : 3.643511045343816\n",
            "Loss for epoch 197 : 3.636259656316505\n",
            "Loss for epoch 198 : 3.629037607742778\n",
            "Loss for epoch 199 : 3.621844738760128\n",
            "Loss for epoch 200 : 3.61468089130082\n",
            "Loss for epoch 201 : 3.6075459099926483\n",
            "Loss for epoch 202 : 3.600439642063417\n",
            "Loss for epoch 203 : 3.5933619372490226\n",
            "Loss for epoch 204 : 3.586312647704944\n",
            "Loss for epoch 205 : 3.579291627921016\n",
            "Loss for epoch 206 : 3.572298734639353\n",
            "Loss for epoch 207 : 3.565333826775279\n",
            "Loss for epoch 208 : 3.558396765341152\n",
            "Loss for epoch 209 : 3.551487413372938\n",
            "Loss for epoch 210 : 3.5446056358594644\n",
            "Loss for epoch 211 : 3.5377512996741953\n",
            "Loss for epoch 212 : 3.5309242735094633\n",
            "Loss for epoch 213 : 3.52412442781304\n",
            "Loss for epoch 214 : 3.517351634726958\n",
            "Loss for epoch 215 : 3.5106057680284897\n",
            "Loss for epoch 216 : 3.5038867030732126\n",
            "Loss for epoch 217 : 3.497194316740062\n",
            "Loss for epoch 218 : 3.4905284873783127\n",
            "Loss for epoch 219 : 3.483889094756397\n",
            "Loss for epoch 220 : 3.47727602001251\n",
            "Loss for epoch 221 : 3.4706891456069258\n",
            "Loss for epoch 222 : 3.4641283552759594\n",
            "Loss for epoch 223 : 3.4575935339875263\n",
            "Loss for epoch 224 : 3.4510845678982327\n",
            "Loss for epoch 225 : 3.4446013443119448\n",
            "Loss for epoch 226 : 3.4381437516397946\n",
            "Loss for epoch 227 : 3.4317116793615616\n",
            "Loss for epoch 228 : 3.4253050179883933\n",
            "Loss for epoch 229 : 3.4189236590268157\n",
            "Loss for epoch 230 : 3.412567494943996\n",
            "Loss for epoch 231 : 3.4062364191342174\n",
            "Loss for epoch 232 : 3.3999303258865203\n",
            "Loss for epoch 233 : 3.393649110353487\n",
            "Loss for epoch 234 : 3.3873926685211337\n",
            "Loss for epoch 235 : 3.3811608971798512\n",
            "Loss for epoch 236 : 3.3749536938964155\n",
            "Loss for epoch 237 : 3.3687709569869795\n",
            "Loss for epoch 238 : 3.3626125854910645\n",
            "Loss for epoch 239 : 3.3564784791464852\n",
            "Loss for epoch 240 : 3.3503685383652275\n",
            "Loss for epoch 241 : 3.3442826642101964\n",
            "Loss for epoch 242 : 3.338220758372871\n",
            "Loss for epoch 243 : 3.332182723151785\n",
            "Loss for epoch 244 : 3.326168461431862\n",
            "Loss for epoch 245 : 3.3201778766645433\n",
            "Loss for epoch 246 : 3.314210872848707\n",
            "Loss for epoch 247 : 3.3082673545123606\n",
            "Loss for epoch 248 : 3.3023472266950713\n",
            "Loss for epoch 249 : 3.296450394931126\n",
            "Loss for epoch 250 : 3.290576765233407\n",
            "Loss for epoch 251 : 3.2847262440779414\n",
            "Loss for epoch 252 : 3.278898738389144\n",
            "Loss for epoch 253 : 3.2730941555256834\n",
            "Loss for epoch 254 : 3.2673124032670184\n",
            "Loss for epoch 255 : 3.2615533898005222\n",
            "Loss for epoch 256 : 3.255817023709229\n",
            "Loss for epoch 257 : 3.2501032139601516\n",
            "Loss for epoch 258 : 3.244411869893175\n",
            "Loss for epoch 259 : 3.2387429012104985\n",
            "Loss for epoch 260 : 3.2330962179666125\n",
            "Loss for epoch 261 : 3.2274717305587934\n",
            "Loss for epoch 262 : 3.2218693497181072\n",
            "Loss for epoch 263 : 3.216288986500898\n",
            "Loss for epoch 264 : 3.210730552280737\n",
            "Loss for epoch 265 : 3.205193958740854\n",
            "Loss for epoch 266 : 3.199679117866984\n",
            "Loss for epoch 267 : 3.1941859419406553\n",
            "Loss for epoch 268 : 3.1887143435328844\n",
            "Loss for epoch 269 : 3.1832642354982683\n",
            "Loss for epoch 270 : 3.1778355309694546\n",
            "Loss for epoch 271 : 3.17242814335199\n",
            "Loss for epoch 272 : 3.1670419863195076\n",
            "Loss for epoch 273 : 3.161676973809268\n",
            "Loss for epoch 274 : 3.156333020018009\n",
            "Loss for epoch 275 : 3.151010039398132\n",
            "Loss for epoch 276 : 3.145707946654158\n",
            "Loss for epoch 277 : 3.1404266567394905\n",
            "Loss for epoch 278 : 3.1351660848534406\n",
            "Loss for epoch 279 : 3.129926146438517\n",
            "Loss for epoch 280 : 3.124706757177963\n",
            "Loss for epoch 281 : 3.1195078329935226\n",
            "Loss for epoch 282 : 3.1143292900434383\n",
            "Loss for epoch 283 : 3.1091710447206604\n",
            "Loss for epoch 284 : 3.1040330136512484\n",
            "Loss for epoch 285 : 3.0989151136929767\n",
            "Loss for epoch 286 : 3.0938172619341073\n",
            "Loss for epoch 287 : 3.088739375692336\n",
            "Loss for epoch 288 : 3.083681372513905\n",
            "Loss for epoch 289 : 3.0786431701728527\n",
            "Loss for epoch 290 : 3.073624686670411\n",
            "Loss for epoch 291 : 3.068625840234533\n",
            "Loss for epoch 292 : 3.0636465493195466\n",
            "Loss for epoch 293 : 3.0586867326059135\n",
            "Loss for epoch 294 : 3.053746309000106\n",
            "Loss for epoch 295 : 3.04882519763457\n",
            "Loss for epoch 296 : 3.0439233178677907\n",
            "Loss for epoch 297 : 3.039040589284432\n",
            "Loss for epoch 298 : 3.034176931695562\n",
            "Loss for epoch 299 : 3.02933226513894\n",
            "Loss for epoch 300 : 3.024506509879376\n",
            "Loss for epoch 301 : 3.0196995864091427\n",
            "Loss for epoch 302 : 3.0149114154484375\n",
            "Loss for epoch 303 : 3.0101419179459024\n",
            "Loss for epoch 304 : 3.005391015079172\n",
            "Loss for epoch 305 : 3.000658628255466\n",
            "Loss for epoch 306 : 2.995944679112211\n",
            "Loss for epoch 307 : 2.9912490895176886\n",
            "Loss for epoch 308 : 2.9865717815717105\n",
            "Loss for epoch 309 : 2.9819126776063025\n",
            "Loss for epoch 310 : 2.977271700186416\n",
            "Loss for epoch 311 : 2.9726487721106407\n",
            "Loss for epoch 312 : 2.9680438164119254\n",
            "Loss for epoch 313 : 2.963456756358312\n",
            "Loss for epoch 314 : 2.9588875154536534\n",
            "Loss for epoch 315 : 2.954336017438346\n",
            "Loss for epoch 316 : 2.9498021862900505\n",
            "Loss for epoch 317 : 2.9452859462243985\n",
            "Loss for epoch 318 : 2.940787221695701\n",
            "Loss for epoch 319 : 2.936305937397634\n",
            "Loss for epoch 320 : 2.9318420182639118\n",
            "Loss for epoch 321 : 2.92739538946895\n",
            "Loss for epoch 322 : 2.9229659764285\n",
            "Loss for epoch 323 : 2.918553704800265\n",
            "Loss for epoch 324 : 2.9141585004845005\n",
            "Loss for epoch 325 : 2.909780289624581\n",
            "Loss for epoch 326 : 2.905418998607547\n",
            "Loss for epoch 327 : 2.901074554064626\n",
            "Loss for epoch 328 : 2.8967468828717213\n",
            "Loss for epoch 329 : 2.8924359121498773\n",
            "Loss for epoch 330 : 2.888141569265708\n",
            "Loss for epoch 331 : 2.883863781831804\n",
            "Loss for epoch 332 : 2.8796024777071\n",
            "Loss for epoch 333 : 2.8753575849972135\n",
            "Loss for epoch 334 : 2.8711290320547564\n",
            "Loss for epoch 335 : 2.866916747479598\n",
            "Loss for epoch 336 : 2.862720660119121\n",
            "Loss for epoch 337 : 2.8585406990684152\n",
            "Loss for epoch 338 : 2.8543767936704603\n",
            "Loss for epoch 339 : 2.8502288735162664\n",
            "Loss for epoch 340 : 2.846096868444978\n",
            "Loss for epoch 341 : 2.8419807085439537\n",
            "Loss for epoch 342 : 2.837880324148809\n",
            "Loss for epoch 343 : 2.833795645843423\n",
            "Loss for epoch 344 : 2.8297266044599225\n",
            "Loss for epoch 345 : 2.8256731310786267\n",
            "Loss for epoch 346 : 2.821635157027963\n",
            "Loss for epoch 347 : 2.817612613884358\n",
            "Loss for epoch 348 : 2.8136054334720915\n",
            "Loss for epoch 349 : 2.8096135478631274\n",
            "Loss for epoch 350 : 2.8056368893769164\n",
            "Loss for epoch 351 : 2.801675390580168\n",
            "Loss for epoch 352 : 2.7977289842866013\n",
            "Loss for epoch 353 : 2.793797603556669\n",
            "Loss for epoch 354 : 2.7898811816972593\n",
            "Loss for epoch 355 : 2.7859796522613682\n",
            "Loss for epoch 356 : 2.782092949047759\n",
            "Loss for epoch 357 : 2.7782210061005994\n",
            "Loss for epoch 358 : 2.7743637577090716\n",
            "Loss for epoch 359 : 2.7705211384069703\n",
            "Loss for epoch 360 : 2.7666930829722878\n",
            "Loss for epoch 361 : 2.762879526426771\n",
            "Loss for epoch 362 : 2.759080404035472\n",
            "Loss for epoch 363 : 2.755295651306287\n",
            "Loss for epoch 364 : 2.7515252039894733\n",
            "Loss for epoch 365 : 2.7477689980771625\n",
            "Loss for epoch 366 : 2.744026969802859\n",
            "Loss for epoch 367 : 2.740299055640938\n",
            "Loss for epoch 368 : 2.7365851923061157\n",
            "Loss for epoch 369 : 2.732885316752942\n",
            "Loss for epoch 370 : 2.729199366175258\n",
            "Loss for epoch 371 : 2.7255272780056656\n",
            "Loss for epoch 372 : 2.7218689899149924\n",
            "Loss for epoch 373 : 2.718224439811745\n",
            "Loss for epoch 374 : 2.7145935658415716\n",
            "Loss for epoch 375 : 2.7109763063867143\n",
            "Loss for epoch 376 : 2.7073726000654688\n",
            "Loss for epoch 377 : 2.7037823857316363\n",
            "Loss for epoch 378 : 2.700205602473986\n",
            "Loss for epoch 379 : 2.6966421896157184\n",
            "Loss for epoch 380 : 2.693092086713925\n",
            "Loss for epoch 381 : 2.689555233559058\n",
            "Loss for epoch 382 : 2.686031570174406\n",
            "Loss for epoch 383 : 2.6825210368155683\n",
            "Loss for epoch 384 : 2.6790235739699426\n",
            "Loss for epoch 385 : 2.675539122356211\n",
            "Loss for epoch 386 : 2.672067622923841\n",
            "Loss for epoch 387 : 2.66860901685259\n",
            "Loss for epoch 388 : 2.66516324555201\n",
            "Loss for epoch 389 : 2.661730250660978\n",
            "Loss for epoch 390 : 2.658309974047218\n",
            "Loss for epoch 391 : 2.6549023578068387\n",
            "Loss for epoch 392 : 2.651507344263879\n",
            "Loss for epoch 393 : 2.648124875969865\n",
            "Loss for epoch 394 : 2.6447548957033673\n",
            "Loss for epoch 395 : 2.641397346469587\n",
            "Loss for epoch 396 : 2.6380521714999214\n",
            "Loss for epoch 397 : 2.6347193142515684\n",
            "Loss for epoch 398 : 2.631398718407122\n",
            "Loss for epoch 399 : 2.6280903278741836\n",
            "Loss for epoch 400 : 2.624794086784981\n",
            "Loss for epoch 401 : 2.621509939495998\n",
            "Loss for epoch 402 : 2.61823783058761\n",
            "Loss for epoch 403 : 2.6149777048637306\n",
            "Loss for epoch 404 : 2.61172950735147\n",
            "Loss for epoch 405 : 2.60849318330079\n",
            "Loss for epoch 406 : 2.6052686781841823\n",
            "Loss for epoch 407 : 2.602055937696344\n",
            "Loss for epoch 408 : 2.5988549077538643\n",
            "Loss for epoch 409 : 2.595665534494916\n",
            "Loss for epoch 410 : 2.5924877642789568\n",
            "Loss for epoch 411 : 2.5893215436864363\n",
            "Loss for epoch 412 : 2.5861668195185055\n",
            "Loss for epoch 413 : 2.5830235387967346\n",
            "Loss for epoch 414 : 2.579891648762839\n",
            "Loss for epoch 415 : 2.5767710968784026\n",
            "Loss for epoch 416 : 2.5736618308246113\n",
            "Loss for epoch 417 : 2.5705637985019854\n",
            "Loss for epoch 418 : 2.567476948030122\n",
            "Loss for epoch 419 : 2.564401227747434\n",
            "Loss for epoch 420 : 2.561336586210892\n",
            "Loss for epoch 421 : 2.5582829721957716\n",
            "Loss for epoch 422 : 2.5552403346954\n",
            "Loss for epoch 423 : 2.5522086229209004\n",
            "Loss for epoch 424 : 2.5491877863009442\n",
            "Loss for epoch 425 : 2.5461777744814937\n",
            "Loss for epoch 426 : 2.5431785373255518\n",
            "Loss for epoch 427 : 2.5401900249129072\n",
            "Loss for epoch 428 : 2.5372121875398777\n",
            "Loss for epoch 429 : 2.5342449757190515\n",
            "Loss for epoch 430 : 2.531288340179028\n",
            "Loss for epoch 431 : 2.5283422318641544\n",
            "Loss for epoch 432 : 2.5254066019342547\n",
            "Loss for epoch 433 : 2.522481401764365\n",
            "Loss for epoch 434 : 2.5195665829444525\n",
            "Loss for epoch 435 : 2.516662097279139\n",
            "Loss for epoch 436 : 2.513767896787413\n",
            "Loss for epoch 437 : 2.5108839337023405\n",
            "Loss for epoch 438 : 2.508010160470771\n",
            "Loss for epoch 439 : 2.505146529753027\n",
            "Loss for epoch 440 : 2.5022929944226044\n",
            "Loss for epoch 441 : 2.4994495075658487\n",
            "Loss for epoch 442 : 2.4966160224816356\n",
            "Loss for epoch 443 : 2.4937924926810373\n",
            "Loss for epoch 444 : 2.490978871886988\n",
            "Loss for epoch 445 : 2.4881751140339325\n",
            "Loss for epoch 446 : 2.485381173267474\n",
            "Loss for epoch 447 : 2.4825970039440115\n",
            "Loss for epoch 448 : 2.4798225606303657\n",
            "Loss for epoch 449 : 2.4770577981033974\n",
            "Loss for epoch 450 : 2.4743026713496246\n",
            "Loss for epoch 451 : 2.471557135564812\n",
            "Loss for epoch 452 : 2.4688211461535756\n",
            "Loss for epoch 453 : 2.466094658728956\n",
            "Loss for epoch 454 : 2.4633776291119944\n",
            "Loss for epoch 455 : 2.4606700133312978\n",
            "Loss for epoch 456 : 2.4579717676225914\n",
            "Loss for epoch 457 : 2.455282848428264\n",
            "Loss for epoch 458 : 2.4526032123968995\n",
            "Loss for epoch 459 : 2.4499328163828085\n",
            "Loss for epoch 460 : 2.4472716174455376\n",
            "Loss for epoch 461 : 2.44461957284938\n",
            "Loss for epoch 462 : 2.4419766400628675\n",
            "Loss for epoch 463 : 2.439342776758263\n",
            "Loss for epoch 464 : 2.436717940811034\n",
            "Loss for epoch 465 : 2.4341020902993225\n",
            "Loss for epoch 466 : 2.431495183503403\n",
            "Loss for epoch 467 : 2.4288971789051326\n",
            "Loss for epoch 468 : 2.426308035187395\n",
            "Loss for epoch 469 : 2.4237277112335285\n",
            "Loss for epoch 470 : 2.421156166126751\n",
            "Loss for epoch 471 : 2.418593359149576\n",
            "Loss for epoch 472 : 2.416039249783216\n",
            "Loss for epoch 473 : 2.413493797706986\n",
            "Loss for epoch 474 : 2.4109569627976875\n",
            "Loss for epoch 475 : 2.408428705128995\n",
            "Loss for epoch 476 : 2.4059089849708277\n",
            "Loss for epoch 477 : 2.4033977627887197\n",
            "Loss for epoch 478 : 2.4008949992431794\n",
            "Loss for epoch 479 : 2.3984006551890364\n",
            "Loss for epoch 480 : 2.3959146916747955\n",
            "Loss for epoch 481 : 2.393437069941973\n",
            "Loss for epoch 482 : 2.390967751424427\n",
            "Loss for epoch 483 : 2.3885066977476854\n",
            "Loss for epoch 484 : 2.386053870728269\n",
            "Loss for epoch 485 : 2.3836092323730025\n",
            "Loss for epoch 486 : 2.3811727448783255\n",
            "Loss for epoch 487 : 2.3787443706295948\n",
            "Loss for epoch 488 : 2.3763240722003878\n",
            "Loss for epoch 489 : 2.3739118123517886\n",
            "Loss for epoch 490 : 2.3715075540316857\n",
            "Loss for epoch 491 : 2.36911126037405\n",
            "Loss for epoch 492 : 2.366722894698216\n",
            "Loss for epoch 493 : 2.364342420508163\n",
            "Loss for epoch 494 : 2.361969801491784\n",
            "Loss for epoch 495 : 2.3596050015201535\n",
            "Loss for epoch 496 : 2.3572479846467957\n",
            "Loss for epoch 497 : 2.354898715106946\n",
            "Loss for epoch 498 : 2.352557157316809\n",
            "Loss for epoch 499 : 2.350223275872817\n",
            "Loss for epoch 500 : 2.3478970355508832\n",
            "Loss for epoch 501 : 2.345578401305652\n",
            "Loss for epoch 502 : 2.343267338269749\n",
            "Loss for epoch 503 : 2.340963811753028\n",
            "Loss for epoch 504 : 2.3386677872418162\n",
            "Loss for epoch 505 : 2.3363792303981574\n",
            "Loss for epoch 506 : 2.334098107059053\n",
            "Loss for epoch 507 : 2.3318243832357037\n",
            "Loss for epoch 508 : 2.329558025112749\n",
            "Loss for epoch 509 : 2.3272989990475\n",
            "Loss for epoch 510 : 2.3250472715691854\n",
            "Loss for epoch 511 : 2.3228028093781767\n",
            "Loss for epoch 512 : 2.3205655793452333\n",
            "Loss for epoch 513 : 2.3183355485107318\n",
            "Loss for epoch 514 : 2.316112684083899\n",
            "Loss for epoch 515 : 2.31389695344205\n",
            "Loss for epoch 516 : 2.311688324129819\n",
            "Loss for epoch 517 : 2.309486763858394\n",
            "Loss for epoch 518 : 2.3072922405047485\n",
            "Loss for epoch 519 : 2.3051047221108747\n",
            "Loss for epoch 520 : 2.3029241768830224\n",
            "Loss for epoch 521 : 2.3007505731909244\n",
            "Loss for epoch 522 : 2.2985838795670377\n",
            "Loss for epoch 523 : 2.2964240647057745\n",
            "Loss for epoch 524 : 2.294271097462737\n",
            "Loss for epoch 525 : 2.292124946853957\n",
            "Loss for epoch 526 : 2.289985582055128\n",
            "Loss for epoch 527 : 2.287852972400843\n",
            "Loss for epoch 528 : 2.285727087383835\n",
            "Loss for epoch 529 : 2.283607896654212\n",
            "Loss for epoch 530 : 2.2814953700187033\n",
            "Loss for epoch 531 : 2.27938947743989\n",
            "Loss for epoch 532 : 2.277290189035456\n",
            "Loss for epoch 533 : 2.275197475077426\n",
            "Loss for epoch 534 : 2.2731113059914128\n",
            "Loss for epoch 535 : 2.271031652355857\n",
            "Loss for epoch 536 : 2.2689584849012823\n",
            "Loss for epoch 537 : 2.2668917745095327\n",
            "Loss for epoch 538 : 2.2648314922130317\n",
            "Loss for epoch 539 : 2.262777609194027\n",
            "Loss for epoch 540 : 2.260730096783845\n",
            "Loss for epoch 541 : 2.258688926462142\n",
            "Loss for epoch 542 : 2.256654069856163\n",
            "Loss for epoch 543 : 2.254625498739996\n",
            "Loss for epoch 544 : 2.2526031850338297\n",
            "Loss for epoch 545 : 2.2505871008032177\n",
            "Loss for epoch 546 : 2.248577218258335\n",
            "Loss for epoch 547 : 2.246573509753244\n",
            "Loss for epoch 548 : 2.2445759477851626\n",
            "Loss for epoch 549 : 2.2425845049937245\n",
            "Loss for epoch 550 : 2.2405991541602566\n",
            "Loss for epoch 551 : 2.2386198682070417\n",
            "Loss for epoch 552 : 2.236646620196599\n",
            "Loss for epoch 553 : 2.2346793833309504\n",
            "Loss for epoch 554 : 2.2327181309509068\n",
            "Loss for epoch 555 : 2.2307628365353396\n",
            "Loss for epoch 556 : 2.228813473700465\n",
            "Loss for epoch 557 : 2.2268700161991246\n",
            "Loss for epoch 558 : 2.2249324379200726\n",
            "Loss for epoch 559 : 2.2230007128872633\n",
            "Loss for epoch 560 : 2.2210748152591395\n",
            "Loss for epoch 561 : 2.219154719327922\n",
            "Loss for epoch 562 : 2.217240399518909\n",
            "Loss for epoch 563 : 2.2153318303897653\n",
            "Loss for epoch 564 : 2.213428986629827\n",
            "Loss for epoch 565 : 2.2115318430593995\n",
            "Loss for epoch 566 : 2.2096403746290587\n",
            "Loss for epoch 567 : 2.2077545564189616\n",
            "Loss for epoch 568 : 2.205874363638148\n",
            "Loss for epoch 569 : 2.203999771623857\n",
            "Loss for epoch 570 : 2.2021307558408356\n",
            "Loss for epoch 571 : 2.2002672918806545\n",
            "Loss for epoch 572 : 2.198409355461029\n",
            "Loss for epoch 573 : 2.196556922425136\n",
            "Loss for epoch 574 : 2.194709968740938\n",
            "Loss for epoch 575 : 2.192868470500509\n",
            "Loss for epoch 576 : 2.191032403919363\n",
            "Loss for epoch 577 : 2.189201745335784\n",
            "Loss for epoch 578 : 2.18737647121016\n",
            "Loss for epoch 579 : 2.185556558124317\n",
            "Loss for epoch 580 : 2.183741982780862\n",
            "Loss for epoch 581 : 2.181932722002522\n",
            "Loss for epoch 582 : 2.1801287527314863\n",
            "Loss for epoch 583 : 2.1783300520287585\n",
            "Loss for epoch 584 : 2.1765365970735013\n",
            "Loss for epoch 585 : 2.174748365162391\n",
            "Loss for epoch 586 : 2.1729653337089756\n",
            "Loss for epoch 587 : 2.171187480243028\n",
            "Loss for epoch 588 : 2.1694147824099113\n",
            "Loss for epoch 589 : 2.1676472179699386\n",
            "Loss for epoch 590 : 2.165884764797745\n",
            "Loss for epoch 591 : 2.164127400881652\n",
            "Loss for epoch 592 : 2.1623751043230435\n",
            "Loss for epoch 593 : 2.160627853335739\n",
            "Loss for epoch 594 : 2.1588856262453744\n",
            "Loss for epoch 595 : 2.157148401488781\n",
            "Loss for epoch 596 : 2.15541615761337\n",
            "Loss for epoch 597 : 2.153688873276521\n",
            "Loss for epoch 598 : 2.1519665272449706\n",
            "Loss for epoch 599 : 2.1502490983942053\n",
            "Loss for epoch 600 : 2.14853656570786\n",
            "Loss for epoch 601 : 2.1468289082771146\n",
            "Loss for epoch 602 : 2.1451261053000943\n",
            "Loss for epoch 603 : 2.1434281360812792\n",
            "Loss for epoch 604 : 2.1417349800309085\n",
            "Loss for epoch 605 : 2.1400466166643923\n",
            "Loss for epoch 606 : 2.1383630256017256\n",
            "Loss for epoch 607 : 2.1366841865669066\n",
            "Loss for epoch 608 : 2.1350100793873517\n",
            "Loss for epoch 609 : 2.133340683993326\n",
            "Loss for epoch 610 : 2.131675980417362\n",
            "Loss for epoch 611 : 2.130015948793691\n",
            "Loss for epoch 612 : 2.1283605693576777\n",
            "Loss for epoch 613 : 2.1267098224452488\n",
            "Loss for epoch 614 : 2.1250636884923373\n",
            "Loss for epoch 615 : 2.123422148034319\n",
            "Loss for epoch 616 : 2.121785181705458\n",
            "Loss for epoch 617 : 2.1201527702383522\n",
            "Loss for epoch 618 : 2.118524894463384\n",
            "Loss for epoch 619 : 2.1169015353081746\n",
            "Loss for epoch 620 : 2.115282673797035\n",
            "Loss for epoch 621 : 2.113668291050428\n",
            "Loss for epoch 622 : 2.112058368284431\n",
            "Loss for epoch 623 : 2.1104528868101964\n",
            "Loss for epoch 624 : 2.108851828033421\n",
            "Loss for epoch 625 : 2.1072551734538183\n",
            "Loss for epoch 626 : 2.105662904664589\n",
            "Loss for epoch 627 : 2.1040750033518982\n",
            "Loss for epoch 628 : 2.1024914512943553\n",
            "Loss for epoch 629 : 2.100912230362494\n",
            "Loss for epoch 630 : 2.0993373225182608\n",
            "Loss for epoch 631 : 2.0977667098144974\n",
            "Loss for epoch 632 : 2.0962003743944364\n",
            "Loss for epoch 633 : 2.094638298491193\n",
            "Loss for epoch 634 : 2.093080464427262\n",
            "Loss for epoch 635 : 2.0915268546140133\n",
            "Loss for epoch 636 : 2.089977451551201\n",
            "Loss for epoch 637 : 2.088432237826463\n",
            "Loss for epoch 638 : 2.086891196114829\n",
            "Loss for epoch 639 : 2.085354309178234\n",
            "Loss for epoch 640 : 2.0838215598650276\n",
            "Loss for epoch 641 : 2.0822929311094955\n",
            "Loss for epoch 642 : 2.080768405931374\n",
            "Loss for epoch 643 : 2.07924796743537\n",
            "Loss for epoch 644 : 2.0777315988106944\n",
            "Loss for epoch 645 : 2.0762192833305777\n",
            "Loss for epoch 646 : 2.0747110043518084\n",
            "Loss for epoch 647 : 2.073206745314261\n",
            "Loss for epoch 648 : 2.071706489740434\n",
            "Loss for epoch 649 : 2.0702102212349858\n",
            "Loss for epoch 650 : 2.068717923484278\n",
            "Loss for epoch 651 : 2.067229580255914\n",
            "Loss for epoch 652 : 2.065745175398291\n",
            "Loss for epoch 653 : 2.0642646928401445\n",
            "Loss for epoch 654 : 2.062788116590099\n",
            "Loss for epoch 655 : 2.061315430736227\n",
            "Loss for epoch 656 : 2.0598466194455978\n",
            "Loss for epoch 657 : 2.0583816669638435\n",
            "Loss for epoch 658 : 2.0569205576147174\n",
            "Loss for epoch 659 : 2.055463275799657\n",
            "Loss for epoch 660 : 2.054009805997355\n",
            "Loss for epoch 661 : 2.0525601327633236\n",
            "Loss for epoch 662 : 2.051114240729469\n",
            "Loss for epoch 663 : 2.0496721146036667\n",
            "Loss for epoch 664 : 2.0482337391693344\n",
            "Loss for epoch 665 : 2.0467990992850167\n",
            "Loss for epoch 666 : 2.045368179883962\n",
            "Loss for epoch 667 : 2.043940965973709\n",
            "Loss for epoch 668 : 2.0425174426356736\n",
            "Loss for epoch 669 : 2.0410975950247368\n",
            "Loss for epoch 670 : 2.039681408368838\n",
            "Loss for epoch 671 : 2.0382688679685677\n",
            "Loss for epoch 672 : 2.0368599591967635\n",
            "Loss for epoch 673 : 2.0354546674981093\n",
            "Loss for epoch 674 : 2.0340529783887367\n",
            "Loss for epoch 675 : 2.0326548774558297\n",
            "Loss for epoch 676 : 2.031260350357227\n",
            "Loss for epoch 677 : 2.029869382821033\n",
            "Loss for epoch 678 : 2.0284819606452285\n",
            "Loss for epoch 679 : 2.0270980696972827\n",
            "Loss for epoch 680 : 2.0257176959137655\n",
            "Loss for epoch 681 : 2.02434082529997\n",
            "Loss for epoch 682 : 2.02296744392953\n",
            "Loss for epoch 683 : 2.0215975379440403\n",
            "Loss for epoch 684 : 2.020231093552683\n",
            "Loss for epoch 685 : 2.018868097031852\n",
            "Loss for epoch 686 : 2.017508534724785\n",
            "Loss for epoch 687 : 2.0161523930411898\n",
            "Loss for epoch 688 : 2.014799658456881\n",
            "Loss for epoch 689 : 2.013450317513413\n",
            "Loss for epoch 690 : 2.0121043568177206\n",
            "Loss for epoch 691 : 2.0107617630417534\n",
            "Loss for epoch 692 : 2.0094225229221228\n",
            "Loss for epoch 693 : 2.008086623259742\n",
            "Loss for epoch 694 : 2.0067540509194766\n",
            "Loss for epoch 695 : 2.0054247928297864\n",
            "Loss for epoch 696 : 2.004098835982382\n",
            "Loss for epoch 697 : 2.0027761674318723\n",
            "Loss for epoch 698 : 2.0014567742954235\n",
            "Loss for epoch 699 : 2.0001406437524105\n",
            "Loss for epoch 700 : 1.99882776304408\n",
            "Loss for epoch 701 : 1.9975181194732106\n",
            "Loss for epoch 702 : 1.9962117004037718\n",
            "Loss for epoch 703 : 1.9949084932605934\n",
            "Loss for epoch 704 : 1.9936084855290295\n",
            "Loss for epoch 705 : 1.9923116647546293\n",
            "Loss for epoch 706 : 1.9910180185428055\n",
            "Loss for epoch 707 : 1.989727534558509\n",
            "Loss for epoch 708 : 1.9884402005259034\n",
            "Loss for epoch 709 : 1.987156004228042\n",
            "Loss for epoch 710 : 1.9858749335065449\n",
            "Loss for epoch 711 : 1.9845969762612818\n",
            "Loss for epoch 712 : 1.9833221204500542\n",
            "Loss for epoch 713 : 1.9820503540882792\n",
            "Loss for epoch 714 : 1.980781665248676\n",
            "Loss for epoch 715 : 1.9795160420609559\n",
            "Loss for epoch 716 : 1.97825347271151\n",
            "Loss for epoch 717 : 1.9769939454431043\n",
            "Loss for epoch 718 : 1.9757374485545718\n",
            "Loss for epoch 719 : 1.974483970400509\n",
            "Loss for epoch 720 : 1.9732334993909753\n",
            "Loss for epoch 721 : 1.9719860239911893\n",
            "Loss for epoch 722 : 1.9707415327212336\n",
            "Loss for epoch 723 : 1.969500014155756\n",
            "Loss for epoch 724 : 1.9682614569236774\n",
            "Loss for epoch 725 : 1.967025849707895\n",
            "Loss for epoch 726 : 1.9657931812449942\n",
            "Loss for epoch 727 : 1.9645634403249583\n",
            "Loss for epoch 728 : 1.9633366157908811\n",
            "Loss for epoch 729 : 1.9621126965386797\n",
            "Loss for epoch 730 : 1.9608916715168132\n",
            "Loss for epoch 731 : 1.9596735297259968\n",
            "Loss for epoch 732 : 1.9584582602189247\n",
            "Loss for epoch 733 : 1.9572458520999891\n",
            "Loss for epoch 734 : 1.956036294525004\n",
            "Loss for epoch 735 : 1.9548295767009296\n",
            "Loss for epoch 736 : 1.9536256878855986\n",
            "Loss for epoch 737 : 1.9524246173874455\n",
            "Loss for epoch 738 : 1.9512263545652322\n",
            "Loss for epoch 739 : 1.9500308888277855\n",
            "Loss for epoch 740 : 1.9488382096337242\n",
            "Loss for epoch 741 : 1.9476483064911985\n",
            "Loss for epoch 742 : 1.9464611689576246\n",
            "Loss for epoch 743 : 1.945276786639421\n",
            "Loss for epoch 744 : 1.9440951491917506\n",
            "Loss for epoch 745 : 1.9429162463182608\n",
            "Loss for epoch 746 : 1.941740067770827\n",
            "Loss for epoch 747 : 1.9405666033492956\n",
            "Loss for epoch 748 : 1.9393958429012326\n",
            "Loss for epoch 749 : 1.9382277763216689\n",
            "Loss for epoch 750 : 1.9370623935528497\n",
            "Loss for epoch 751 : 1.935899684583987\n",
            "Loss for epoch 752 : 1.9347396394510097\n",
            "Loss for epoch 753 : 1.9335822482363196\n",
            "Loss for epoch 754 : 1.9324275010685443\n",
            "Loss for epoch 755 : 1.9312753881222973\n",
            "Loss for epoch 756 : 1.9301258996179325\n",
            "Loss for epoch 757 : 1.9289790258213078\n",
            "Loss for epoch 758 : 1.9278347570435432\n",
            "Loss for epoch 759 : 1.9266930836407878\n",
            "Loss for epoch 760 : 1.9255539960139783\n",
            "Loss for epoch 761 : 1.9244174846086113\n",
            "Loss for epoch 762 : 1.9232835399145047\n",
            "Loss for epoch 763 : 1.922152152465571\n",
            "Loss for epoch 764 : 1.9210233128395848\n",
            "Loss for epoch 765 : 1.919897011657955\n",
            "Loss for epoch 766 : 1.9187732395854973\n",
            "Loss for epoch 767 : 1.9176519873302098\n",
            "Loss for epoch 768 : 1.9165332456430473\n",
            "Loss for epoch 769 : 1.915417005317699\n",
            "Loss for epoch 770 : 1.9143032571903678\n",
            "Loss for epoch 771 : 1.9131919921395484\n",
            "Loss for epoch 772 : 1.91208320108581\n",
            "Loss for epoch 773 : 1.9109768749915779\n",
            "Loss for epoch 774 : 1.9098730048609183\n",
            "Loss for epoch 775 : 1.9087715817393223\n",
            "Loss for epoch 776 : 1.9076725967134935\n",
            "Loss for epoch 777 : 1.9065760409111352\n",
            "Loss for epoch 778 : 1.9054819055007393\n",
            "Loss for epoch 779 : 1.9043901816913773\n",
            "Loss for epoch 780 : 1.9033008607324924\n",
            "Loss for epoch 781 : 1.9022139339136892\n",
            "Loss for epoch 782 : 1.901129392564533\n",
            "Loss for epoch 783 : 1.9000472280543406\n",
            "Loss for epoch 784 : 1.89896743179198\n",
            "Loss for epoch 785 : 1.8978899952256665\n",
            "Loss for epoch 786 : 1.8968149098427618\n",
            "Loss for epoch 787 : 1.895742167169576\n",
            "Loss for epoch 788 : 1.894671758771167\n",
            "Loss for epoch 789 : 1.8936036762511435\n",
            "Loss for epoch 790 : 1.8925379112514706\n",
            "Loss for epoch 791 : 1.8914744554522724\n",
            "Loss for epoch 792 : 1.8904133005716397\n",
            "Loss for epoch 793 : 1.8893544383654368\n",
            "Loss for epoch 794 : 1.88829786062711\n",
            "Loss for epoch 795 : 1.887243559187497\n",
            "Loss for epoch 796 : 1.8861915259146371\n",
            "Loss for epoch 797 : 1.885141752713585\n",
            "Loss for epoch 798 : 1.8840942315262215\n",
            "Loss for epoch 799 : 1.8830489543310676\n",
            "Loss for epoch 800 : 1.8820059131431006\n",
            "Loss for epoch 801 : 1.8809651000135712\n",
            "Loss for epoch 802 : 1.879926507029817\n",
            "Loss for epoch 803 : 1.8788901263150848\n",
            "Loss for epoch 804 : 1.8778559500283476\n",
            "Loss for epoch 805 : 1.8768239703641254\n",
            "Loss for epoch 806 : 1.8757941795523074\n",
            "Loss for epoch 807 : 1.8747665698579739\n",
            "Loss for epoch 808 : 1.8737411335812177\n",
            "Loss for epoch 809 : 1.872717863056973\n",
            "Loss for epoch 810 : 1.871696750654836\n",
            "Loss for epoch 811 : 1.8706777887788946\n",
            "Loss for epoch 812 : 1.8696609698675541\n",
            "Loss for epoch 813 : 1.8686462863933675\n",
            "Loss for epoch 814 : 1.8676337308628623\n",
            "Loss for epoch 815 : 1.8666232958163709\n",
            "Loss for epoch 816 : 1.865614973827866\n",
            "Loss for epoch 817 : 1.8646087575047858\n",
            "Loss for epoch 818 : 1.8636046394878745\n",
            "Loss for epoch 819 : 1.8626026124510102\n",
            "Loss for epoch 820 : 1.8616026691010434\n",
            "Loss for epoch 821 : 1.8606048021776325\n",
            "Loss for epoch 822 : 1.8596090044530782\n",
            "Loss for epoch 823 : 1.8586152687321655\n",
            "Loss for epoch 824 : 1.8576235878519967\n",
            "Loss for epoch 825 : 1.856633954681835\n",
            "Loss for epoch 826 : 1.855646362122944\n",
            "Loss for epoch 827 : 1.8546608031084284\n",
            "Loss for epoch 828 : 1.8536772706030746\n",
            "Loss for epoch 829 : 1.8526957576031966\n",
            "Loss for epoch 830 : 1.8517162571364771\n",
            "Loss for epoch 831 : 1.8507387622618139\n",
            "Loss for epoch 832 : 1.849763266069163\n",
            "Loss for epoch 833 : 1.848789761679388\n",
            "Loss for epoch 834 : 1.8478182422441038\n",
            "Loss for epoch 835 : 1.846848700945527\n",
            "Loss for epoch 836 : 1.8458811309963237\n",
            "Loss for epoch 837 : 1.8449155256394574\n",
            "Loss for epoch 838 : 1.843951878148043\n",
            "Loss for epoch 839 : 1.842990181825193\n",
            "Loss for epoch 840 : 1.8420304300038746\n",
            "Loss for epoch 841 : 1.8410726160467565\n",
            "Loss for epoch 842 : 1.8401167333460677\n",
            "Loss for epoch 843 : 1.8391627753234476\n",
            "Loss for epoch 844 : 1.8382107354298036\n",
            "Loss for epoch 845 : 1.8372606071451645\n",
            "Loss for epoch 846 : 1.8363123839785394\n",
            "Loss for epoch 847 : 1.8353660594677728\n",
            "Loss for epoch 848 : 1.834421627179402\n",
            "Loss for epoch 849 : 1.8334790807085193\n",
            "Loss for epoch 850 : 1.8325384136786262\n",
            "Loss for epoch 851 : 1.8315996197414972\n",
            "Loss for epoch 852 : 1.8306626925770402\n",
            "Loss for epoch 853 : 1.8297276258931554\n",
            "Loss for epoch 854 : 1.8287944134256005\n",
            "Loss for epoch 855 : 1.827863048937852\n",
            "Loss for epoch 856 : 1.826933526220969\n",
            "Loss for epoch 857 : 1.8260058390934575\n",
            "Loss for epoch 858 : 1.8250799814011354\n",
            "Loss for epoch 859 : 1.824155947016998\n",
            "Loss for epoch 860 : 1.8232337298410848\n",
            "Loss for epoch 861 : 1.8223133238003455\n",
            "Loss for epoch 862 : 1.8213947228485083\n",
            "Loss for epoch 863 : 1.8204779209659485\n",
            "Loss for epoch 864 : 1.8195629121595565\n",
            "Loss for epoch 865 : 1.8186496904626086\n",
            "Loss for epoch 866 : 1.8177382499346362\n",
            "Loss for epoch 867 : 1.8168285846612964\n",
            "Loss for epoch 868 : 1.8159206887542463\n",
            "Loss for epoch 869 : 1.8150145563510114\n",
            "Loss for epoch 870 : 1.8141101816148606\n",
            "Loss for epoch 871 : 1.81320755873468\n",
            "Loss for epoch 872 : 1.812306681924846\n",
            "Loss for epoch 873 : 1.8114075454250986\n",
            "Loss for epoch 874 : 1.8105101435004218\n",
            "Loss for epoch 875 : 1.8096144704409125\n",
            "Loss for epoch 876 : 1.8087205205616628\n",
            "Loss for epoch 877 : 1.8078282882026349\n",
            "Loss for epoch 878 : 1.8069377677285379\n",
            "Loss for epoch 879 : 1.806048953528709\n",
            "Loss for epoch 880 : 1.805161840016989\n",
            "Loss for epoch 881 : 1.8042764216316045\n",
            "Loss for epoch 882 : 1.803392692835047\n",
            "Loss for epoch 883 : 1.8025106481139535\n",
            "Loss for epoch 884 : 1.8016302819789878\n",
            "Loss for epoch 885 : 1.8007515889647245\n",
            "Loss for epoch 886 : 1.799874563629527\n",
            "Loss for epoch 887 : 1.7989992005554347\n",
            "Loss for epoch 888 : 1.7981254943480458\n",
            "Loss for epoch 889 : 1.7972534396363984\n",
            "Loss for epoch 890 : 1.7963830310728606\n",
            "Loss for epoch 891 : 1.79551426333301\n",
            "Loss for epoch 892 : 1.7946471311155252\n",
            "Loss for epoch 893 : 1.7937816291420672\n",
            "Loss for epoch 894 : 1.7929177521571706\n",
            "Loss for epoch 895 : 1.7920554949281275\n",
            "Loss for epoch 896 : 1.7911948522448793\n",
            "Loss for epoch 897 : 1.7903358189199003\n",
            "Loss for epoch 898 : 1.7894783897880935\n",
            "Loss for epoch 899 : 1.7886225597066738\n",
            "Loss for epoch 900 : 1.7877683235550632\n",
            "Loss for epoch 901 : 1.7869156762347786\n",
            "Loss for epoch 902 : 1.7860646126693234\n",
            "Loss for epoch 903 : 1.785215127804082\n",
            "Loss for epoch 904 : 1.7843672166062086\n",
            "Loss for epoch 905 : 1.783520874064522\n",
            "Loss for epoch 906 : 1.7826760951893994\n",
            "Loss for epoch 907 : 1.7818328750126688\n",
            "Loss for epoch 908 : 1.780991208587505\n",
            "Loss for epoch 909 : 1.7801510909883234\n",
            "Loss for epoch 910 : 1.7793125173106767\n",
            "Loss for epoch 911 : 1.778475482671149\n",
            "Loss for epoch 912 : 1.777639982207255\n",
            "Loss for epoch 913 : 1.7768060110773338\n",
            "Loss for epoch 914 : 1.7759735644604502\n",
            "Loss for epoch 915 : 1.7751426375562889\n",
            "Loss for epoch 916 : 1.7743132255850544\n",
            "Loss for epoch 917 : 1.773485323787373\n",
            "Loss for epoch 918 : 1.7726589274241855\n",
            "Loss for epoch 919 : 1.771834031776656\n",
            "Loss for epoch 920 : 1.771010632146064\n",
            "Loss for epoch 921 : 1.7701887238537117\n",
            "Loss for epoch 922 : 1.7693683022408198\n",
            "Loss for epoch 923 : 1.768549362668436\n",
            "Loss for epoch 924 : 1.7677319005173313\n",
            "Loss for epoch 925 : 1.7669159111879065\n",
            "Loss for epoch 926 : 1.7661013901000946\n",
            "Loss for epoch 927 : 1.7652883326932642\n",
            "Loss for epoch 928 : 1.764476734426125\n",
            "Loss for epoch 929 : 1.763666590776631\n",
            "Loss for epoch 930 : 1.7628578972418867\n",
            "Loss for epoch 931 : 1.7620506493380534\n",
            "Loss for epoch 932 : 1.7612448426002536\n",
            "Loss for epoch 933 : 1.760440472582481\n",
            "Loss for epoch 934 : 1.759637534857502\n",
            "Loss for epoch 935 : 1.758836025016769\n",
            "Loss for epoch 936 : 1.758035938670327\n",
            "Loss for epoch 937 : 1.7572372714467182\n",
            "Loss for epoch 938 : 1.756440018992896\n",
            "Loss for epoch 939 : 1.7556441769741324\n",
            "Loss for epoch 940 : 1.7548497410739266\n",
            "Loss for epoch 941 : 1.7540567069939177\n",
            "Loss for epoch 942 : 1.753265070453793\n",
            "Loss for epoch 943 : 1.7524748271912016\n",
            "Loss for epoch 944 : 1.7516859729616638\n",
            "Loss for epoch 945 : 1.750898503538485\n",
            "Loss for epoch 946 : 1.7501124147126665\n",
            "Loss for epoch 947 : 1.7493277022928193\n",
            "Loss for epoch 948 : 1.7485443621050791\n",
            "Loss for epoch 949 : 1.7477623899930157\n",
            "Loss for epoch 950 : 1.7469817818175528\n",
            "Loss for epoch 951 : 1.7462025334568778\n",
            "Loss for epoch 952 : 1.7454246408063603\n",
            "Loss for epoch 953 : 1.7446480997784655\n",
            "Loss for epoch 954 : 1.7438729063026706\n",
            "Loss for epoch 955 : 1.7430990563253823\n",
            "Loss for epoch 956 : 1.7423265458098527\n",
            "Loss for epoch 957 : 1.7415553707360953\n",
            "Loss for epoch 958 : 1.7407855271008061\n",
            "Loss for epoch 959 : 1.7400170109172772\n",
            "Loss for epoch 960 : 1.7392498182153182\n",
            "Loss for epoch 961 : 1.7384839450411744\n",
            "Loss for epoch 962 : 1.7377193874574461\n",
            "Loss for epoch 963 : 1.7369561415430075\n",
            "Loss for epoch 964 : 1.736194203392928\n",
            "Loss for epoch 965 : 1.7354335691183924\n",
            "Loss for epoch 966 : 1.73467423484662\n",
            "Loss for epoch 967 : 1.7339161967207886\n",
            "Loss for epoch 968 : 1.733159450899954\n",
            "Loss for epoch 969 : 1.7324039935589743\n",
            "Loss for epoch 970 : 1.7316498208884301\n",
            "Loss for epoch 971 : 1.730896929094548\n",
            "Loss for epoch 972 : 1.7301453143991257\n",
            "Loss for epoch 973 : 1.7293949730394522\n",
            "Loss for epoch 974 : 1.7286459012682354\n",
            "Loss for epoch 975 : 1.727898095353524\n",
            "Loss for epoch 976 : 1.7271515515786342\n",
            "Loss for epoch 977 : 1.726406266242072\n",
            "Loss for epoch 978 : 1.7256622356574636\n",
            "Loss for epoch 979 : 1.7249194561534755\n",
            "Loss for epoch 980 : 1.7241779240737467\n",
            "Loss for epoch 981 : 1.7234376357768102\n",
            "Loss for epoch 982 : 1.7226985876360232\n",
            "Loss for epoch 983 : 1.7219607760394944\n",
            "Loss for epoch 984 : 1.7212241973900102\n",
            "Loss for epoch 985 : 1.7204888481049654\n",
            "Loss for epoch 986 : 1.7197547246162883\n",
            "Loss for epoch 987 : 1.719021823370374\n",
            "Loss for epoch 988 : 1.7182901408280098\n",
            "Loss for epoch 989 : 1.7175596734643066\n",
            "Loss for epoch 990 : 1.7168304177686295\n",
            "Loss for epoch 991 : 1.7161023702445268\n",
            "Loss for epoch 992 : 1.715375527409662\n",
            "Loss for epoch 993 : 1.7146498857957437\n",
            "Loss for epoch 994 : 1.7139254419484593\n",
            "Loss for epoch 995 : 1.7132021924274028\n",
            "Loss for epoch 996 : 1.7124801338060118\n",
            "Loss for epoch 997 : 1.7117592626714964\n",
            "Loss for epoch 998 : 1.7110395756247734\n",
            "Loss for epoch 999 : 1.7103210692804003\n",
            "Loss for epoch 1000 : 1.709603740266507\n",
            "Test Accuracy: 52.39 %\n",
            "[[613  17  43  70  46  22 124   5  59   1]\n",
            " [ 10 766  30 128  17  13  17  11   8   0]\n",
            " [ 35   9 389  32 252  30 208   0  43   2]\n",
            " [ 86 120  23 574  25  13 112  14  33   0]\n",
            " [ 40  34 243  94 379  21 161   1  23   4]\n",
            " [  7   9  45  16  25 341  24 253 144 136]\n",
            " [188  23 136  69 238  31 229   5  77   4]\n",
            " [  4   1   3  16   0 107   5 686  36 142]\n",
            " [ 24   9  74  10  38 174  59  29 552  31]\n",
            " [  4   0  14   0   0 143  10  77  42 710]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd70ec1dda0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncP8flU+ogQosZEcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_QXwIGVtZfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "711fdf9d-837d-497f-dccd-13066dd2360e"
      },
      "source": [
        "fig = plt.gcf()\n",
        "plt.plot(training_loss, '--m', linestyle='--', markersize=1)\n",
        "plt.plot(validation_loss, '--b', linestyle='--', markersize=1)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "fig.savefig('training_validation_loss_task1.png', dpi=100)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncP8flU+ogQosZEcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ms-jATszlPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}