{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFTEbHbuI4kY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Created on 10/17/19 @ 16:54:10\n",
        "\n",
        "@author: ajithkumar-natarajan\n",
        "\"\"\"\n",
        "\n",
        "#############################TASK 1\n",
        "# Necessary imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import keras\n",
        "\n",
        "#Function to load data\n",
        "def load_mnist(path, kind='train'):\n",
        "\timport os\n",
        "\timport gzip\n",
        "\n",
        "\t\"\"\"Load MNIST data from `path`\"\"\"\n",
        "\tlabels_path = os.path.join(path,\n",
        "                               '%s-labels-idx1-ubyte.gz'\n",
        "                               % kind)\n",
        "\timages_path = os.path.join(path,\n",
        "                               '%s-images-idx3-ubyte.gz'\n",
        "                               % kind)\n",
        "\twith gzip.open(labels_path, 'rb') as lbpath:\n",
        "\t\tlabels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "                               offset=8)\n",
        "\n",
        "\twith gzip.open(images_path, 'rb') as imgpath:\n",
        "\t\timages = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "                               offset=16).reshape(len(labels), 784)\n",
        "\n",
        "\treturn images, labels\n",
        "\n",
        "#Function to calculate loss\n",
        "def calculate_loss(y, a2):\n",
        "  # loss_sum = np.sum(np.multiply(y, np.log(a2)))\n",
        "  # length = y.shape[1]\n",
        "  # normalized_loss = -(1/length) * loss_sum\n",
        "  return np.sum(-np.log(a2[range(y.shape[0]),y]))/y.shape[0]\n",
        "\n",
        "  return normalized_loss\n",
        "\n",
        "#Function to return sigmoid of input\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "#initializing constants\n",
        "classes = 10\n",
        "training_dataset_count = 60000\n",
        "test_dataset_count = 10000\n",
        "\n",
        "#reading data\n",
        "x_train, y_train = load_mnist('data/fashion', kind='train')\n",
        "x_test, y_test = load_mnist('data/fashion', kind='t10k')\n",
        "\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "# print(y_train.shape)\n",
        "# print(y_test.shape)\n",
        "\n",
        "#Normalizing data\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "\n",
        "\n",
        "#Training model\n",
        "y_train = y_train.reshape(1, training_dataset_count)\n",
        "y_test = y_test.reshape(1, test_dataset_count)\n",
        "\n",
        "y_new_training = np.eye(classes)[y_train.astype('int32')]\n",
        "y_new_test = np.eye(classes)[y_test.astype('int32')]\n",
        "\n",
        "y_new_training = y_new_training.T.reshape(classes, training_dataset_count)\n",
        "y_new_test = y_new_test.T.reshape(classes, test_dataset_count)\n",
        "\n",
        "x_train = x_train.T\n",
        "x_test = x_test.T\n",
        "\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "# print(y_new_training.shape)\n",
        "# print(y_new_test.shape)\n",
        "\n",
        "#Randomising dataset before training\n",
        "np.random.seed(100)\n",
        "shuffle_index = np.random.permutation(training_dataset_count)\n",
        "x_train, y_train = x_train[:, shuffle_index], y_train[:, shuffle_index]\n",
        "\n",
        "\n",
        "i = 13\n",
        "plt.imshow(x_train[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "print(\"label-> \", y_train[:,i])\n",
        "\n",
        "\n",
        "n_x = x_train.shape[0]\n",
        "n_h = 64\n",
        "learning_rate = 1\n",
        "\n",
        "\n",
        "#Forward propagation\n",
        "W1 = np.random.randn(n_h, n_x)\n",
        "b1 = np.zeros((n_h, 1))\n",
        "W2 = np.random.randn(classes, n_h)\n",
        "b2 = np.zeros((classes, 1))\n",
        "\n",
        "x = x_train\n",
        "y = y_train\n",
        "\n",
        "# (x_train, y_train) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# print(\"-----\")\n",
        "# print(len(x_train))\n",
        "# print(len(y_train))\n",
        "\n",
        "# m = 60000\n",
        "# m_test = x_final.shape[0] - m\n",
        "\n",
        "# X_train, X_test = x_train[:m].T, x_train[m:].T\n",
        "# Y_train, Y_test = y_train[:,:m], y_train[:,m:]\n",
        "\n",
        "# x = X_train\n",
        "# y = Y_train\n",
        "\n",
        "\n",
        "#Iterating and training the model over 500 epochs\n",
        "for i in range(500):\n",
        "  Z1 = np.matmul(W1,x) + b1\n",
        "  A1 = sigmoid(Z1)\n",
        "  Z2 = np.matmul(W2,A1) + b2\n",
        "  A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
        "\n",
        "  cost = compute_multiclass_loss(y, A2)\n",
        "\n",
        "  dZ2 = A2-y\n",
        "  dW2 = (1./training_dataset_count) * np.matmul(dZ2, A1.T)\n",
        "  db2 = (1./training_dataset_count) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "  dA1 = np.matmul(W2.T, dZ2)\n",
        "  dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
        "  dW1 = (1./training_dataset_count) * np.matmul(dZ1, x.T)\n",
        "  db1 = (1./training_dataset_count) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "  W2 = W2 - learning_rate * dW2\n",
        "  b2 = b2 - learning_rate * db2\n",
        "  W1 = W1 - learning_rate * dW1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "\n",
        "  if (i % 100 == 0):\n",
        "    print(y, A2)\n",
        "    print(\"Epoch\", i, \"cost: \", cost)\n",
        "\n",
        "print(\"Final cost:\", cost)\n",
        "\n",
        "\n",
        "# Z1 = np.matmul(W1, x_test) + b1\n",
        "# A1 = sigmoid(Z1)\n",
        "# Z2 = np.matmul(W2, A1) + b2\n",
        "# A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
        "\n",
        "# predictions = np.argmax(A2, axis=0)\n",
        "# labels = np.argmax(y_test, axis=0)\n",
        "\n",
        "#Predicting test data\n",
        "Z1 = np.matmul(W1, X_test) + b1\n",
        "A1 = sigmoid(Z1)\n",
        "Z2 = np.matmul(W2, A1) + b2\n",
        "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
        "\n",
        "predictions = np.argmax(A2, axis=0)\n",
        "labels = np.argmax(Y_test, axis=0)\n",
        "\n",
        "#Printing confusion matrix\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))\n",
        "\n",
        "\n",
        "#############################TASK 2\n",
        "\n",
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# LOG_DIR = './log'\n",
        "# get_ipython().system_raw(\n",
        "#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "#     .format(LOG_DIR)\n",
        "# )\n",
        "\n",
        "# get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n",
        "#necessary imports\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "# import pickle\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "\n",
        "# def load_mnist(path, kind='train'):\n",
        "# \timport os\n",
        "# \timport gzip\n",
        "# \timport numpy as np\n",
        "\n",
        "# \t\"\"\"Load MNIST data from `path`\"\"\"\n",
        "# \tlabels_path = os.path.join(path,\n",
        "#                                '%s-labels-idx1-ubyte.gz'\n",
        "#                                % kind)\n",
        "# \timages_path = os.path.join(path,\n",
        "#                                '%s-images-idx3-ubyte.gz'\n",
        "#                                % kind)\n",
        "# \twith gzip.open(labels_path, 'rb') as lbpath:\n",
        "# \t\tlabels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "#                                offset=8)\n",
        "\n",
        "# \twith gzip.open(images_path, 'rb') as imgpath:\n",
        "# \t\timages = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "#                                offset=16).reshape(len(labels), 784)\n",
        "\n",
        "# \treturn images, labels\n",
        "\n",
        "# x_train, y_train = load_mnist('data/fashion', kind='train')\n",
        "# x_test, y_test = load_mnist('data/fashion', kind='t10k')\n",
        "\n",
        "#Loading dataset from keras\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "#setting batch size\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "# print(type(x_train))\n",
        "\n",
        "# x_train = x_train.reshape(x_train, (60000, 28, 28 ))\n",
        "# x_test = x_test.reshape(x_test, (60000, 28, 28))\n",
        "\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "\n",
        "# i = 13\n",
        "# plt.imshow(x_train[i,:,:], cmap = matplotlib.cm.binary)\n",
        "# plt.axis(\"off\")\n",
        "# plt.show()\n",
        "# print(\"label-> \", y_train[i])\n",
        "\n",
        "#normalizing data\n",
        "x_train = x_train.astype('float32')/255\n",
        "x_test = x_test.astype('float32')/255\n",
        "\n",
        "#One-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_orig = y_test\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "#Stacking up layers for neural network\n",
        "model = keras.Sequential([keras.layers.Flatten(input_shape=(28,28)), \n",
        "                          # keras.layers.Dense(400, activation='sigmoid'), \n",
        "                          keras.layers.Dense(128, activation='relu'), \n",
        "                          keras.layers.Dense(10, activation='softmax')])\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# tbCallBack = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "#                          write_graph=True,\n",
        "#                          write_grads=True,\n",
        "#                          batch_size=batch_size,\n",
        "#                          write_images=True)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=batch_size, validation_data=(x_test, y_test))\n",
        "# with open('trainHistoryDict', 'wb') as file_pi:\n",
        "        # pickle.dump(output.history, file_pi)\n",
        "\n",
        "output = model.predict_classes(x_test)\n",
        "# con_mat = tf.math.confusion_matrix(labels=y_test, predictions=output).numpy()\n",
        "# con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        " \n",
        "# con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "#                      index = classes, \n",
        "#                      columns = classes)\n",
        "\n",
        "#Printing confusion matrix\n",
        "confusion_matrix = confusion_matrix(y_test_orig, output)\n",
        "print(confusion_matrix)\n",
        "\n",
        "\n",
        "#############################TASK 3\n",
        "#Importing necessaryy libraries\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plot\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# def load_mnist(path, kind='train'):\n",
        "# \timport os\n",
        "# \timport gzip\n",
        "\n",
        "# \t\"\"\"Load MNIST data from `path`\"\"\"\n",
        "# \tlabels_path = os.path.join(path,\n",
        "#                                '%s-labels-idx1-ubyte.gz'\n",
        "#                                % kind)\n",
        "# \timages_path = os.path.join(path,\n",
        "#                                '%s-images-idx3-ubyte.gz'\n",
        "#                                % kind)\n",
        "# \twith gzip.open(labels_path, 'rb') as lbpath:\n",
        "# \t\tlabels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
        "#                                offset=8)\n",
        "\n",
        "# \twith gzip.open(images_path, 'rb') as imgpath:\n",
        "# \t\timages = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
        "#                                offset=16).reshape(len(labels), 784)\n",
        "\n",
        "# \treturn images, labels\n",
        "\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "#Loading dataset\n",
        "x_train, y_train = load_mnist('data/fashion', kind='train')\n",
        "x_test, y_test = load_mnist('data/fashion', kind='t10k')\n",
        "\n",
        "#Getting data ready for CNN training\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test_orig = y_test\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "#Building the CNN model\n",
        "model = keras.Sequential([keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape = (28, 28, 1)), \n",
        "                          keras.layers.pooling.MaxPooling2D(pool_size = (2, 2)), \n",
        "                          keras.layers.Conv2D(64, (3, 3), activation='relu'), \n",
        "                          keras.layers.pooling.MaxPool2D(pool_size = (2, 2)), \n",
        "                          keras.layers.Dropout(0.25), \n",
        "                          keras.layers.Flatten(), \n",
        "                          keras.layers.Dense(256, activation='relu'), \n",
        "                          keras.layers.Dropout(0.5), \n",
        "                          keras.layers.Dense(10, activation='softmax')])\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "#Predicting output\n",
        "output = model.predict_classes(x_test)\n",
        "\n",
        "#Printing confusion matrix\n",
        "confusion_matrix = confusion_matrix(y_test_orig, output)\n",
        "print(confusion_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}